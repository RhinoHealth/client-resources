{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rhino_inference_helpers import (\n",
    "    load_tokenizer_and_model, \n",
    "    read_data, \n",
    "    inference, \n",
    "    create_dataset, \n",
    "    read_output_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the radiology reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the input report file from /input/dataset.csv, where a dataset has been pre-loaded via FCP\n",
    "reports = read_data()\n",
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first report\n",
    "report = reports['report_content'][0].strip()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Tokenizer and Model Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the model artifacts. Use /external_data in case of reading data from S3\n",
    "model_artifacts = \"/external_data/MY_MODEL\"\n",
    "tokenizer, model = load_tokenizer_and_model(model_artifacts,device_map = \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference in Q&A Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your question here\n",
    "responses = []\n",
    "for i,report in enumerate(reports['report_content']):\n",
    "    input_text = 'Extract clinical entities from the text. \\\n",
    "                  Do not extract negative mention of an entity. \\\n",
    "                  Identify current findings and historical mentions: \\\n",
    "                  Format the response as JSON \\\n",
    "                  \\n\\n' + report\n",
    "    responses.append(inference(input_text, tokenizer, model))\n",
    "    print(f'Report number {i+1} analyzed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Responses post-processing\n",
    "responses_as_json = []\n",
    "for response in responses:\n",
    "    responses_as_json.append(json.loads(response))\n",
    "\n",
    "# Display the first LLM output\n",
    "print(responses_as_json[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Dataset for FCP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new column with desired information from LLM inference\n",
    "reports['responses'] = responses_as_json\n",
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save as a new dataset\n",
    "create_dataset(reports)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify the output dataset\n",
    "\n",
    "output_df = read_output_datasets()\n",
    "output_df\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}